{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYZ9oJ9b1xZ7"
   },
   "source": [
    "## Prática Guiada Requests e Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo IMDB\n",
    "\n",
    "**Raspando dados para mais de 2000 filmes**\n",
    "\n",
    "\n",
    "Queremos analisar as distribuições das classificações de filmes do IMDB e do Metacritic para ver se encontramos algo interessante. Para fazer isso, primeiro vamos coletar dados para mais de 2000 filmes.\n",
    "\n",
    "É essencial identificar o objetivo da nossa raspagem desde o início. Escrever um script de raspagem pode levar muito tempo, especialmente se quisermos raspar mais de uma página da web. Queremos evitar passar horas escrevendo um script que raspe dados que realmente não precisaremos.\n",
    "\n",
    "**Trabalhando em quais páginas raspar**\n",
    "\n",
    "\n",
    "Uma vez que estabelecemos nosso objetivo, precisamos identificar um conjunto eficiente de páginas para raspar.\n",
    "\n",
    "Queremos encontrar uma combinação de páginas que requer um número relativamente pequeno de solicitações. Uma solicitação é o que acontece sempre que acessamos uma página da web. Nós \"solicitamos\" o conteúdo de uma página do servidor. Quanto mais pedidos fizermos, mais tempo o script precisará executar e maior será a sobrecarga do servidor.\n",
    "\n",
    "Uma forma de obter todos os dados de que precisamos é compilar uma lista de nomes de filmes e usá-los para acessar a página da web de cada filme nos sites do IMDB e do Metacritic.\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/2017/12/option1.gif'>\n",
    "<br>\n",
    "Como queremos obter mais de 2.000 classificações do IMDB e do Metacritic, teremos que fazer pelo menos 4.000 solicitações. Se fizermos uma solicitação por segundo, nosso script precisará de um pouco mais de uma hora para fazer 4000 solicitações. Por isso, vale a pena tentar identificar maneiras mais eficientes de obter nossos dados.\n",
    "\n",
    "Se explorarmos o site do IMDB, podemos descobrir uma maneira de reduzir o número de solicitações pela metade. As pontuações do Metacritic são mostradas na página do filme do IMDB, por isso podemos recapitular ambas as classificações com um único pedido:\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/option2.jpg'>\n",
    "<br>\n",
    "<br>\n",
    "Se investigarmos o site do IMDB, poderemos descobrir a página mostrada abaixo. Ele contém todos os dados que precisamos para 50 filmes. Dado o nosso objetivo, isso significa que só teremos que fazer 40 pedidos, o que é 100 vezes menor que a nossa primeira opção. Vamos explorar mais esta última opção.\n",
    "\n",
    "<img src='https://www.dataquest.io/blog/content/images/option3.jpg'>\n",
    "<br>\n",
    "<br>\n",
    "**Identificando a estrutura do URL**\n",
    "<br>\n",
    "<br>\n",
    "Nosso desafio agora é ter certeza de que entendemos a lógica do URL como as páginas que queremos para alterar a mudança. Se não conseguirmos entender essa lógica o suficiente para podermos implementá-la em código, chegaremos a um beco sem saída.\n",
    "<br>\n",
    "Se você for na página de pesquisa avançada do IMDB , você pode procurar filmes por ano:\n",
    "<br>\n",
    "\n",
    "<img src='https://www.dataquest.io/blog/content/images/advanced_search.png'>\n",
    "\n",
    "Vamos navegar pelo ano de 2017, ordenar os filmes na primeira página por número de votos, depois mudar para a próxima página. Chegaremos a esta página da web , que tem este URL:\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='https://www.dataquest.io/blog/content/images/url.png'>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Na imagem acima, você pode ver que o URL tem vários parâmetros após o ponto de interrogação:\n",
    "\n",
    "* `release_date` - Mostra apenas os filmes lançados em um ano específico.\n",
    "* `sort` - Classifica os filmes na página. sort=num_votes,desctraduz para classificar por número de votos em ordem decrescente .\n",
    "* `page` - Especifica o número da página.\n",
    "* `ref_` - Leva-nos para a próxima página ou a anterior. A referência é a página em que estamos atualmente. adv_nxte adv_prvsão dois valores possíveis. Eles traduzem para avançar para a próxima página e avançar para a página anterior , respectivamente.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Se você navegar por essas páginas e observar o URL, perceberá que apenas os valores dos parâmetros mudam. Isso significa que podemos escrever um script para corresponder à lógica das alterações e fazer muito menos solicitações para coletar nossos dados.\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Vamos começar a escrever o script, solicitando o conteúdo desta página web único: http://www.imdb.com/search/title?release_date=2017&sort=num_votes,desc&page=1. Na seguinte célula de código nós iremos:\n",
    "\n",
    "Importe a get()função do requestsmódulo.\n",
    "Atribua o endereço da página da web a uma variável nomeada url.\n",
    "Solicite ao servidor o conteúdo da página da Web usando get()e armazene a resposta do servidor na variável response.\n",
    "Imprima uma pequena parte do responseconteúdo acessando seu .textatributo ( responseagora é um Responseobjeto)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "\n",
    "url = 'http://www.imdb.com/search/title?release_date=2017&sort=num_votes,desc&page=1'\n",
    "\n",
    "response = get(url)\n",
    "print(response.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entendendo a estrutura HTML de uma única página\n",
    "\n",
    "<br>\n",
    "<img src='https://cdn-images-1.medium.com/max/1000/0*ETFzXPCNHkPpqNv_.png'>\n",
    "<br>\n",
    "Como você pode ver na primeira linha response.text, o servidor nos enviou um documento HTML. Este documento descreve a estrutura geral dessa página da Web, juntamente com seu conteúdo específico (que é o que torna essa página específica única).\n",
    "\n",
    "Todas as páginas que queremos copiar têm a mesma estrutura geral. Isso implica que eles também têm a mesma estrutura HTML geral. Então, para escrever nosso script, será suficiente entender a estrutura HTML de apenas uma página. Para fazer isso, usaremos as ferramentas de desenvolvedor do navegador .\n",
    "\n",
    "Se você usa o Chrome , clique com o botão direito do mouse em um elemento da página da web que lhe interessa e clique em Inspecionar . Isso levará você diretamente para a linha HTML que corresponde a esse elemento:\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/inspecthtml.png'>\n",
    "<br>\n",
    "\n",
    "Clique com o botão direito do mouse no nome do filme e, em seguida, clique com o botão esquerdo do mouse em Inspecionar . A linha HTML destacada em cinza corresponde ao que o usuário vê na página da Web como o nome do filme.\n",
    "\n",
    "Você também pode fazer isso usando o Firefox e o Safari DevTools.\n",
    "\n",
    "Observe que todas as informações de cada filme, incluindo o pôster, estão contidas em uma divtag.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/one_container.jpg'>\n",
    "<br>\n",
    "\n",
    "Existem muitas linhas HTML aninhadas em cada divtag. Você pode explorá-las clicando nas pequenas setas cinzas à esquerda das linhas HTML correspondentes a cada uma delas div. Dentro dessas tags aninhadas, encontraremos as informações de que precisamos, como a classificação de um filme.\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/movie_rating.jpg'>\n",
    "<br>\n",
    "\n",
    "Há 50 filmes exibidos por página, portanto, deve haver um divcontêiner para cada um. Vamos extrair todos esses 50 contêineres analisando o documento HTML de nossa solicitação anterior.\n",
    "\n",
    "# Usando BeautifulSoup para analisar o conteúdo HTML\n",
    "Para analisar nosso documento HTML e extrair os 50 divcontêineres, usaremos um módulo Python chamado BeautifulSoup , o módulo de raspagem da Web mais comum para Python.\n",
    "\n",
    "Na seguinte célula de código nós iremos:\n",
    "\n",
    "- Importe o `BeautifulSoupcriador` da classe do pacote bs4.\n",
    "- Analise `response.text` criando um `BeautifulSoup` objeto e atribua esse objeto a `html_soup`. O `html.parser` argumento indica que queremos fazer a análise usando o [analisador HTML do Python](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_soup = BeautifulSoup(response.text, 'html.parser')\n",
    "html_soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de extrair os 50 contêineres, precisamos descobrir o que os diferencia de outros divelementos nessa página. Muitas vezes, a marca distintiva reside no class atributo . Se você inspecionar as linhas HTML dos contêineres de interesse, notará que o classatributo tem dois valores: lister-iteme mode-advanced. Essa combinação é exclusiva desses divcontêineres. Podemos ver que isso é verdade fazendo uma pesquisa rápida ( Ctrl + F). Temos 50 desses contêineres, então esperamos ver apenas 50 correspondências:\n",
    "\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/search.jpg'>\n",
    "<br>\n",
    "\n",
    "Agora vamos usar o find_all() método para extrair todos os divcontainers que possuem um classatributo lister-item mode-advanced:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_containers = html_soup.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "print(type(movie_containers))\n",
    "print(len(movie_containers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for movie in movie_containers:\n",
    "#     print(movie)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O `find_all()` retornou um `ResultSet` objeto que é uma lista contendo todos os 50 divs que nos interessam.\n",
    "\n",
    "Agora vamos selecionar apenas o primeiro container e extrair, por sua vez, cada item de interesse:\n",
    "\n",
    "- O nome do filme\n",
    "- O ano do lançamento.\n",
    "- A classificação do IMDB.\n",
    "- O Metascore.\n",
    "- O número de votos.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/datapoints.jpg'>\n",
    "<br>\n",
    "\n",
    "\n",
    "# Extraindo os dados para um único filme\n",
    "Podemos acessar o primeiro contêiner, que contém informações sobre um único filme, usando notação de lista em movie_containers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_movie = movie_containers[0]\n",
    "first_movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como você pode ver, o conteúdo HTML de um contêiner é muito longo. Para descobrir a linha HTML específica para cada ponto de dados, usaremos o DevTools novamente.\n",
    "\n",
    "\n",
    "# O nome do filme\n",
    "\n",
    "Começamos com o nome do filme e localizamos sua linha HTML correspondente usando o DevTools. Você pode ver que o nome está contido em uma tag de âncora ( a). Esta tag é aninhada dentro de uma tag de cabeçalho (h3). A h3 tag está aninhada em uma div tag. Este div é o terceiro do divsaninhado no container do primeiro filme. Nós armazenamos o conteúdo desse contêiner na first_movie variável.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/movie_name.jpg'>\n",
    "<br>\n",
    "\n",
    "first_movieé um Tag objeto , e as várias tags HTML dentro dele são armazenadas como seus atributos. Podemos acessá-los como se tivéssemos acesso a qualquer atributo de um objeto Python. No entanto, usar um nome de tag como um atributo selecionará apenas a primeira tag com esse nome. Se corrermos first_movie.div, só obtemos o conteúdo da primeira divtag:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_movie.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acessar a primeira tag âncora (**a**) não nos leva ao nome do filme. O primeiro **a** está em algum lugar dentro do segundo div:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_movie.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No entanto, acessar a primeira **h3** tag nos aproxima muito:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_movie.h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui, podemos usar a notação de atributo para acessar o primeiro **a** dentro da **h3** tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_movie.h3.a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora tudo é apenas uma questão de acessar o texto de dentro dessa **a** :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_name = first_movie.h3.a.text\n",
    "first_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O ano do lançamento do filme\n",
    "Continuamos com a extração do ano. Esses dados são armazenados na **span** abaixo do **a** que contém o nome.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/year_name.png'>\n",
    "<br>\n",
    "\n",
    "A notação de pontos só acessará o primeiro spanelemento. Vamos procurar pela marca distintiva do segundo **span**. Nós vamos usar o `find()` método que é quase o mesmo que `find_all()`, exceto que só retorna a primeira partida. Na verdade, `find()` é equivalente a `find_all(limit = 1)`. O limit argumento limita a saída para a primeira correspondência.\n",
    "\n",
    "A marca distintiva consiste nos valores lister-item-year text-muted unboldatribuídos ao classatributo. Então procuramos o primeiro **span** com esses valores dentro da **h3**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = first_movie.h3.find('span', class_ = 'lister-item-year text-muted unbold')\n",
    "first_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir daqui, basta acessar o texto usando a notação de atributo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = first_year.text\n",
    "first_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poderíamos facilmente limpar essa saída e convertê-la em um inteiro. Mas, se você explorar mais páginas, perceberá que, para alguns filmes, o ano aceita valores imprevisíveis como (2017) (I) ou (2015) (V). É mais eficiente fazer a limpeza depois da raspagem, quando saberemos todos os valores do ano.\n",
    "\n",
    "# A classificação do IMDB\n",
    "Agora nos concentramos em extrair a classificação do IMDB do primeiro filme.\n",
    "\n",
    "Existem algumas maneiras de fazer isso, mas primeiro tentaremos a mais fácil. Se você inspecionar a classificação do IMDB usando DevTools, perceberá que a classificação está contida em uma **strong**.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/imdb_rating.png'>\n",
    "<br>\n",
    "\n",
    "\n",
    "Vamos usar a notação de atributo e esperar que o primeiro **strong** também seja o que contenha a classificação.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_movie.strong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ótimo! Vamos acessar o texto, convertê-lo para o `float`tipo e atribuí-lo à variável `first_imdb`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_imdb = float(first_movie.strong.text)\n",
    "first_imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O Metascore\n",
    "Se nós inspecionarmos o Metascore usando DevTools, notamos que podemos encontrá-lo dentro de uma **span**.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/metascore.png'>\n",
    "<br>\n",
    "\n",
    "A notação de atributos claramente não é uma solução. Existem muitas **span** antes disso. Você pode ver um logo acima da **strong**. É melhor usarmos os valores distintivos do classatributo ( metascore favorable).\n",
    "\n",
    "Observe que, se você copiar e colar esses valores da guia DevTools, haverá dois caracteres de espaço em branco entre metascoree favorable. Certifique-se de que haverá apenas um caractere de espaço em branco quando você passar os valores como argumentos. Caso contrário, `find()` não encontrará nada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_mscore = first_movie.find('span', class_ = 'metascore favorable')\n",
    "\n",
    "first_mscore = int(first_mscore.text)\n",
    "print(first_mscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O favorablevalor indica um alto Metascore e define a cor de fundo da classificação como verde. Os outros dois valores possíveis são unfavorablee mixed. O que é específico para todas as classificações do Metascore é apenas o metascorevalor. Este é o que vamos usar quando vamos escrever o script para a página inteira.\n",
    "\n",
    "# O número de votos\n",
    "O número de votos está contido em uma <span>tag. Sua marca distintiva é um nameatributo com o valor nv.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/nr_votes.png'>\n",
    "<br>\n",
    "\n",
    "\n",
    "O name atributo é diferente do class atributo. Usando BeautifulSoup podemos acessar elementos por qualquer atributo. As funções `find()` e `find_all()` têm um parâmetro chamado `attrs`. Para isso, podemos passar os atributos e valores que procuramos como dicionário:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_votes = first_movie.find('span', attrs = {'name':'nv'})\n",
    "first_votes.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poderíamos usar a .text notação para acessar o <span>conteúdo da tag. Seria melhor se nós acessássemos o valor do data-valueatributo. Desta forma, podemos converter o ponto de dados extraído para um intsem ter que retirar uma vírgula.\n",
    "\n",
    "Você pode tratar um Tagobjeto como um dicionário. Os atributos HTML são as chaves do dicionário. Os valores dos atributos HTML são os valores das chaves do dicionário. É assim que podemos acessar o valor do data-valueatributo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_votes['data-value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos converter esse valor para um inteiro e atribuí-lo a first_votes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_votes = int(first_votes['data-value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É isso aí! Estamos agora em posição de escrever facilmente um script para criar uma única página.\n",
    "\n",
    "O script para uma única página\n",
    "Antes de juntar o que fizemos até agora, temos que ter certeza de que extrairemos os dados apenas dos contêineres que possuem um Metascore.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/no_mscores.jpg'>\n",
    "<br>\n",
    "\n",
    "Precisamos adicionar uma condição para ignorar filmes sem um Metascore.\n",
    "\n",
    "Usando DevTools novamente, vemos que a seção Metascore está contida em uma **div**. O class atributo tem dois valores: inline-blocke ratings-metascore. O distintivo é claro ratings-metascore.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/metascore_yes.png'>\n",
    "<br>\n",
    "\n",
    "\n",
    "Podemos usar find() para pesquisar cada contêiner de filme por divter essa marca distinta. Quando find() não encontra nada, retorna um Noneobjeto. Podemos usar esse resultado em uma ifinstrução para controlar se um filme é copiado.\n",
    "\n",
    "Vamos procurar na [página da web](https://www.imdb.com/search/title?release_date=2017&sort=num_votes,desc&page=1) para procurar por um contêiner de filme que não tenha um Metascore e ver o que find() retorna.\n",
    "\n",
    "Importante: quando eu corri o seguinte código, o oitavo contêiner não tinha um Metascore. No entanto, este é um alvo em movimento, porque o número de votos muda constantemente para cada filme. Para obter as mesmas saídas que na próxima célula de código demonstrativo, você deve pesquisar um contêiner que não tenha um Metascore no momento em que estiver executando o código."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie22_mscore = movie_containers[22].find('div', class_ = 'ratings-metascore')\n",
    "type(movie22_mscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie22_mscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos montar o código acima e comprimir o máximo possível, mas apenas na medida em que ele ainda é facilmente legível. No próximo bloco de código nós:\n",
    "\n",
    "- Declare algumas listas de variáveis para ter algo para armazenar os dados extraídos.\n",
    "- Faça um loop por cada contêiner movie_containers(a variável que contém todos os 50 contêineres de filme).\n",
    "- Extraia os pontos de dados de interesse somente se o contêiner tiver um Metascore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the scraped data in\n",
    "names = []\n",
    "years = []\n",
    "imdb_ratings = []\n",
    "metascores = []\n",
    "votes = []\n",
    "\n",
    "# Extract data from individual movie container\n",
    "for container in movie_containers:\n",
    "\n",
    "    # If the movie has Metascore, then extract:\n",
    "    if container.find('div', class_ = 'ratings-metascore') is not None:\n",
    "\n",
    "        # The name\n",
    "        name = container.h3.a.text\n",
    "        names.append(name)\n",
    "\n",
    "        # The year\n",
    "        year = container.h3.find('span', class_ = 'lister-item-year').text\n",
    "        years.append(year)\n",
    "\n",
    "        # The IMDB rating\n",
    "        imdb = float(container.strong.text)\n",
    "        imdb_ratings.append(imdb)\n",
    "\n",
    "        # The Metascore\n",
    "        m_score = container.find('span', class_ = 'metascore').text\n",
    "        metascores.append(int(m_score))\n",
    "\n",
    "        # The number of votes\n",
    "        vote = container.find('span', attrs = {'name':'nv'})['data-value']\n",
    "        votes.append(int(vote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(votes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos verificar os dados coletados até o momento. Os pandas facilitam para nós ver se coletamos nossos dados com sucesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_df = pd.DataFrame({'movie': names,\n",
    "                       'year': years,\n",
    "                       'imdb': imdb_ratings,\n",
    "                       'metascore': metascores,\n",
    "                       'votes': votes})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tudo correu como esperado!\n",
    "\n",
    "Como observação, se você executar o código de um país onde o inglês não é o idioma principal, é muito provável que você obtenha alguns dos nomes dos filmes traduzidos para o idioma principal desse país.\n",
    "\n",
    "Provavelmente, isso acontece porque o servidor infere sua localização do seu endereço IP. Mesmo se você estiver em um país onde o inglês é o idioma principal, talvez você ainda receba conteúdo traduzido. Isso pode acontecer se você estiver usando uma VPN enquanto faz as GETsolicitações.\n",
    "\n",
    "Se você encontrar esse problema, passe os seguintes valores para o headers parâmetro da função get():\n",
    "\n",
    "\n",
    "# O script para várias páginas\n",
    "Raspagem de várias páginas é um pouco mais desafiador. Nós vamos construir sobre o nosso script de uma página, fazendo mais três coisas:\n",
    "\n",
    "Fazendo todos os pedidos que queremos dentro do loop.\n",
    "Controlando a taxa do loop para evitar bombardear o servidor com solicitações.\n",
    "Monitorando o loop enquanto ele é executado.\n",
    "Nós vamos raspar as primeiras 4 páginas de cada ano no intervalo de 2000-2017. 4 páginas para cada um dos 18 anos perfazem um total de 72 páginas. Cada página tem 50 filmes, por isso vamos buscar dados para 3600 filmes no máximo. Mas nem todos os filmes têm um Metascore, então o número será menor do que isso. Mesmo assim, ainda estamos muito propensos a obter dados para mais de 2000 filmes.\n",
    "\n",
    "# Alterando os parâmetros da URL\n",
    "Como mostrado anteriormente, as URLs seguem uma certa lógica conforme as páginas da web mudam.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/url.png'>\n",
    "<br>\n",
    "\n",
    "Como estamos fazendo as solicitações, precisaremos apenas variar os valores de apenas dois parâmetros da URL: o release_dateparâmetro e page. Vamos preparar os valores que precisaremos para o próximo ciclo. Na próxima célula de código nós iremos:\n",
    "\n",
    "- Crie uma lista chamada pages e preencha-a com as sequências correspondentes às primeiras 4 páginas.\n",
    "- Crie uma lista chamada years_url e preencha-a com as strings correspondentes aos anos 2000-2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [str(i) for i in range(1,5)]\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_url = [str(i) for i in range(2015,2018)]\n",
    "years_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlando a crawl-rate\n",
    "\n",
    "Controlar a taxa de rastreamento é benéfico para nós e para o site que estamos raspando. Se evitarmos martelar o servidor com dezenas de solicitações por segundo, é muito menos provável que nosso endereço IP seja banido. Também evitamos interromper a atividade do site que criamos, permitindo que o servidor responda também às solicitações de outros usuários.\n",
    "\n",
    "Controlaremos a taxa do loop usando a sleep() função do time módulo do Python. sleep() fará uma pausa na execução do loop por um período especificado de segundos.\n",
    "\n",
    "Para imitar o comportamento humano, vamos variar a quantidade de tempo de espera entre as solicitações usando a randint() função do random módulo do Python . randint()aleatoriamente gera inteiros dentro de um intervalo especificado.\n",
    "\n",
    "<br>\n",
    "<img src='https://www.dataquest.io/blog/content/images/2017/12/sleep_new.gif'>\n",
    "<br>\n",
    "\n",
    "Por enquanto, vamos apenas importar essas duas funções para evitar superlotação na célula de código que contém nosso loop principal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitorando o loop como ele ainda está indo\n",
    "Dado que estamos raspando 72 páginas, seria bom se pudéssemos encontrar uma maneira de monitorar o processo de raspagem como ele ainda está indo. Esse recurso é definitivamente opcional, mas pode ser muito útil no processo de teste e depuração. Além disso, quanto maior o número de páginas, mais útil será o monitoramento. Se você for raspar centenas ou milhares de páginas da web em uma única execução de código, eu diria que esse recurso se torna uma obrigação.\n",
    "\n",
    "Para nosso script, usaremos esse recurso e monitoraremos os seguintes parâmetros:\n",
    "\n",
    "- A frequência (velocidade) das solicitações , portanto, garantimos que nosso programa não sobrecarregue o servidor.\n",
    "- O número de pedidos , para que possamos interromper o loop caso o número de solicitações esperadas seja excedido.\n",
    "- O código de status de nossas solicitações, portanto, garantimos que o servidor esteja enviando as respostas adequadas.\n",
    "\n",
    "Para obter um valor de frequência, dividimos o número de solicitações pelo tempo decorrido desde a primeira solicitação. Isso é semelhante ao cálculo da velocidade de um carro - dividimos a distância pelo tempo gasto para cobrir essa distância. Vamos experimentar primeiro essa técnica de monitoramento em pequena escala. Na seguinte célula de código nós iremos:\n",
    "\n",
    "1. Defina uma hora de início usando a time() função do time módulo e atribua o valor a start_time.\n",
    "2. Atribua 0 à variável requestsque usaremos para contar o número de solicitações.\n",
    "3. Inicie um loop e, em seguida, a cada iteração:\n",
    "4. Simule um pedido.\n",
    "5. Incrementar o número de solicitações por 1.\n",
    "6. Pause o loop por um intervalo de tempo entre 8 e 15 segundos.\n",
    "7. Calcule o tempo decorrido desde a primeira solicitação e atribua o valor a elapsed_time.\n",
    "8. Imprima o número de pedidos e a frequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "start_time = time()\n",
    "requests = 0\n",
    "\n",
    "for _ in range(5):\n",
    "    # A request would go here\n",
    "    requests += 1\n",
    "    sleep(randint(1,3))\n",
    "    elapsed_time = time() - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vamos fazer 72 requests, nosso trabalho parecerá um pouco desordenado à medida que a saída se acumular. Para evitar isso, limparemos a saída após cada iteração e a substituiremos por informações sobre a solicitação mais recente. Para fazer isso, usaremos a clear_output()função do core.display módulo do IPython . Vamos definir o parâmetro wait de clear_output() para True esperar com a substituição da saída atual até que apareça alguma nova saída."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import clear_output\n",
    "\n",
    "start_time = time()\n",
    "requests = 0\n",
    "\n",
    "for _ in range(5):\n",
    "    # A request would go here\n",
    "    requests += 1\n",
    "    sleep(randint(1,3))\n",
    "    current_time = time()\n",
    "    elapsed_time = current_time - start_time\n",
    "    print('Request: {}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "    clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para monitorar o código de status, definiremos o programa para nos avisar se algo estiver desligado. Uma solicitação bem-sucedida é indicada por um código de status de 200. Usaremos a warn() função do warnings módulo para lançar um aviso se o código de status não for 200.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import warn\n",
    "\n",
    "warn(\"Warning Simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolhemos um aviso sobre a quebra do loop, porque há uma boa possibilidade de obter dados suficientes, mesmo que algumas das solicitações falhem. Vamos apenas quebrar o loop se o número de solicitações for maior que o esperado.\n",
    "\n",
    "# Juntando tudo\n",
    "Agora vamos juntar tudo o que fizemos até agora! Na célula de código a seguir, começamos por:\n",
    "\n",
    "- Redeclarando as variáveis de listas para que elas se tornem vazias novamente.\n",
    "- Preparando o monitoramento do loop.\n",
    "\n",
    "**Então nós vamos:**\n",
    "\n",
    "    Percorra a years_urllista para variar o release_dateparâmetro do URL.\n",
    "    Para cada elemento years_url, percorra a pageslista para variar o pageparâmetro da URL.\n",
    "    Faça os GETpedidos dentro do pagesloop.\n",
    "    Pause o loop por um intervalo de tempo entre 8 e 15 segundos.\n",
    "    Monitore cada solicitação como discutido anteriormente.\n",
    "    Lance um aviso para códigos de status não-200.\n",
    "    Quebre o loop se o número de solicitações for maior que o esperado.\n",
    "    Converta o responseconteúdo HTML de um BeautifulSoupobjeto.\n",
    "    Extraia todos os contêineres de filmes desse BeautifulSoupobjeto.\n",
    "    Faça um loop por todos esses contêineres.\n",
    "    Extraia os dados se um contêiner tiver um Metascore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redeclaring the lists to store data in\n",
    "names = []\n",
    "years = []\n",
    "imdb_ratings = []\n",
    "metascores = []\n",
    "votes = []\n",
    "\n",
    "# Preparing the monitoring of the loop\n",
    "start_time = time()\n",
    "requests = 0\n",
    "\n",
    "# For every year in the interval 2015-2017\n",
    "for year_url in years_url:\n",
    "\n",
    "    # For every page in the interval 1-4\n",
    "    for page in pages:\n",
    "\n",
    "        # Make a get request\n",
    "        response = get('http://www.imdb.com/search/title?release_date=' \\\n",
    "                       + str(year_url) + \n",
    "        '&sort=num_votes,desc&page=' + str(page))\n",
    "\n",
    "        # Pause the loop\n",
    "        sleep(randint(1,4))\n",
    "\n",
    "        # Monitor the requests\n",
    "        requests += 1\n",
    "        elapsed_time = time() - start_time\n",
    "        print('Request:{}; Frequency: {} requests/s'.format(requests, requests/elapsed_time))\n",
    "        clear_output(wait = True)\n",
    "\n",
    "        # Throw a warning for non-200 status codes\n",
    "        if response.status_code != 200:\n",
    "            warn('Request: {}; Status code: {}'.format(requests, response.status_code))\n",
    "\n",
    "        # Break the loop if the number of requests is greater than expected\n",
    "        if requests > 72:\n",
    "            warn('Number of requests was greater than expected.')  \n",
    "            break \n",
    "\n",
    "        # Parse the content of the request with BeautifulSoup\n",
    "        page_html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Select all the 50 movie containers from a single page\n",
    "        mv_containers = page_html.find_all('div', class_ = 'lister-item mode-advanced')\n",
    "\n",
    "        # For every movie of these 50\n",
    "        for container in mv_containers:\n",
    "            # If the movie has a Metascore, then:\n",
    "            if container.find('div', class_ = 'ratings-metascore') is not None:\n",
    "\n",
    "                # Scrape the name\n",
    "                name = container.h3.a.text\n",
    "                names.append(name)\n",
    "\n",
    "                # Scrape the year \n",
    "                year = container.h3.find('span', class_ = 'lister-item-year').text\n",
    "                years.append(year)\n",
    "\n",
    "                # Scrape the IMDB rating\n",
    "                imdb = float(container.strong.text)\n",
    "                imdb_ratings.append(imdb)\n",
    "\n",
    "                # Scrape the Metascore\n",
    "                m_score = container.find('span', class_ = 'metascore').text\n",
    "                metascores.append(int(m_score))\n",
    "\n",
    "                # Scrape the number of votes\n",
    "                vote = container.find('span', attrs = {'name':'nv'})['data-value']\n",
    "                votes.append(int(vote))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examinando os dados raspados\n",
    "No próximo bloco de código nós:\n",
    "\n",
    "Mesclar os dados em um pandas DataFrame.\n",
    "Imprima algumas informações sobre o recém criado DataFrame.\n",
    "Mostre as 10 primeiras entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings = pd.DataFrame({'movie': names,\n",
    "                              'year': years,\n",
    "                              'imdb': imdb_ratings,\n",
    "                              'metascore': metascores,\n",
    "                              'votes': votes})\n",
    "\n",
    "movie_ratings.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_ratings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemplo 2\n",
    "\n",
    "Utilizando bs4 para pegar dados do campeonato Argentino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9lv5ptoP1xZ-"
   },
   "source": [
    "Nesta Prática Guiada usaremos BeautifulSoup para baixar a informação da primeira divisão de futebol da Argentina da página da ESPN:\n",
    "\n",
    "    http://www.espn.com.ar/futbol/posiciones/_/liga/arg.\n",
    "\n",
    "Devemos baixá-la, extraí-la, ordená-la e guardá-la em um csv e/ou um DataFrame para seu uso. \n",
    "Primeiro, importamos as bibliotecas necessárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MKBr6w7w1xZ_"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "#Com isto, podemos ver o código HTML diretamente no Notebook\n",
    "from IPython.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tgl_HAJR1xaF"
   },
   "source": [
    "Para verificar se não há problema scrapeando esta informação, revisamos o arquivo \"robots.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "T7OWV-eX1xaG",
    "outputId": "9bd20013-c549-4e5d-f4ac-2ac525b969d3"
   },
   "outputs": [],
   "source": [
    "robots = 'http://www.espn.com.ar/' + 'robots.txt'\n",
    "print(requests.get(robots).text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E_LSwR8x1xaN"
   },
   "source": [
    "Nossa URL não aparece, então procedemos.\n",
    "\n",
    "Fazemos um request \"GET\" ao URL com a tabela e vemos o conteúdo da resposta no Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "vN3N9kof1xaN"
   },
   "outputs": [],
   "source": [
    "url = 'http://www.espn.com.ar/futbol/posiciones/_/liga/arg.1'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "2TXZujGc1xaR",
    "outputId": "42bf7326-b1d1-44f5-bb7e-583a21bd1a32",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Vemos o conteúdo renderizado\n",
    "# display(HTML(resp.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HEbXsFL71xaX"
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1PqiE90a1xaa"
   },
   "source": [
    "Podemos acessar \"tags\" diretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xigRTWKe1xac",
    "outputId": "d3110c19-c9ee-4718-9fbb-baa1df219499"
   },
   "outputs": [],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUFv0qov1xah"
   },
   "source": [
    "Para trabalhar com o conteúdo, vamos usar alguns dos seguintes métodos que este objeto tem, como:\n",
    "\n",
    "- .findAll() / .find_all\n",
    "- .find()\n",
    "- .get()\n",
    "- .get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ik0-gska1xai",
    "outputId": "148c5e6e-5b84-4e94-8605-5e675ee8b5d4"
   },
   "outputs": [],
   "source": [
    "soup.head.link.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0igGNb8h1xam"
   },
   "outputs": [],
   "source": [
    "#Podemos usar find() para encontrar a tabela\n",
    "\n",
    "raw_table = soup.find('table', {'class':'standings has-team-logos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "a-OnX5yz1xar",
    "outputId": "d1e84d68-179e-42f4-fa80-20fd6c2399ff"
   },
   "outputs": [],
   "source": [
    "type(raw_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHCrrVp91xaw"
   },
   "source": [
    "Isto nos retorna um objeto \"Tag\" definido pela biblioteca BeautifulSoup. Podemos ver seus atributos usando .attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rPuKpnob1xax",
    "outputId": "46ea5e2d-8a59-48c7-8798-56394cb056a4"
   },
   "outputs": [],
   "source": [
    "raw_table.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zk3FSq3M1xa3"
   },
   "source": [
    "Dentro do atributo \"children\" encontramos todos os descendentes desse nó. No caso da nossa tabela, os descendentes são tags \"tr\", que correspondem a cada fila. Existe outro atributo chamado \"descendants\", que, ao contrário do primeiro, é recursivo. Neste caso, só precisamos dos nós diretamente próximos, então usamos o primeiro método.\n",
    "\n",
    "Iteramos todas as filas e formamos uma matriz com os dados, para depois carregá-los a um DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "GLAFHzoK1xa3",
    "outputId": "82354b3e-a8ad-4880-c681-2cc8d4d19dfa"
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for row in raw_table.children:\n",
    "    rows.append(row.get_text(separator= ','))\n",
    "\n",
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rPorMp7m1xa-"
   },
   "outputs": [],
   "source": [
    "#Separamos as colunas e descartamos as filas vazias\n",
    "table = [row.split(',') for row in rows if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3qQS7rSv1xbC",
    "outputId": "b15b1d2e-8709-4bb5-8972-f65cc819ef5c"
   },
   "outputs": [],
   "source": [
    "#Corrigimos a tabela\n",
    "\n",
    "table[0] = ['Index', 'Name', 'Abbr'] + table[0][1:]\n",
    "print(*table, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vwPuNFrp1xbJ"
   },
   "source": [
    "Para guardar o arquivo a um csv usando Python, usamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(table[1:], columns= table[0])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "02lv1Y-01xbK"
   },
   "outputs": [],
   "source": [
    "filename = 'tabla.csv'\n",
    "with open(filename, 'w') as out:\n",
    "    out.write('\\n'.join([','.join(row) for row in table]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fCYubdLY1xbN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "QUH4I3r11xbS",
    "outputId": "b7841adb-3dc4-4f38-ee8e-28cc1e3afa69"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1iF9aVJ01xbX",
    "outputId": "86f3405a-ef69-4af5-a09c-7315c36ba2f0"
   },
   "outputs": [],
   "source": [
    "df.set_index('Index', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9LIa1tXS1xbb"
   },
   "source": [
    "Por último, podemos guardar o arquivo usando pandas com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lKH8miSV1xbb"
   },
   "outputs": [],
   "source": [
    "filename = 'tabela_ex.xlsx'\n",
    "df.to_excel(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercicio\n",
    "\n",
    "Captura uma lista de noticias **e** urls das noticias da pagina principal do site de noticias.\n",
    "\n",
    "obs: tente não acessar os href para não bloquearem nosso ip ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Desafio\n",
    "\n",
    "Crie um programa que pegue os preços dos notebooks diariamente no site walmart e avise quando o preço diminuir mais que 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "PRACTICA_GUIADA_Requests_BeautifulSoup_pt_br.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
